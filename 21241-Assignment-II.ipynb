{"cells":[{"cell_type":"markdown","metadata":{"id":"OoJzdmJdomPJ"},"source":["## Review Questions-I from MDSC-301(P)\n","\n","----------------------------------------------------------------\n","Author:Thunga Srihari\n","\n","Date: August 28, 2022\n","\n","----------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"Ps2LFOb0omPN"},"source":["Q1. Which Linear Regression training algorithm can you use if you have\n","a training set with millions of features?"]},{"cell_type":"markdown","source":["#we can use mini-batch gradient descent."],"metadata":{"id":"b96IiwQ6pItI"}},{"cell_type":"markdown","metadata":{"id":"v6yFWReaomPN"},"source":["Q2. Suppose the features in your training set have very different scales.\n","Which algorithms might suffer from this, and how? What can you\n","do about it?"]},{"cell_type":"markdown","source":["##The normal equations method does not require normalizing the features, so it remains unaffected by features in the training set having very different scales.\n","\n","##Feature scaling is required for the various gradient descent algorithms. Feature scaling will help gradient descent converge quicker."],"metadata":{"id":"d-IuypK1pY7J"}},{"cell_type":"markdown","metadata":{"id":"W-fbvReUomPN"},"source":["Q3.  Suppose you use Batch Gradient Descent and you plot the validation\n","error at every epoch. If you notice that the validation error\n","consistently goes up, what is likely going on? How can you fix this?"]},{"cell_type":"markdown","source":["##When we use Batch Gradient Descent,suppose the validation error increasing or  goes up,its an indication that the model could be diverging because of high learning rate. \n","##If the training error also increases or goes up, that is the indication of diverging. This can be fixed by lowering the learning rate and then re-training. \n","##If the training error is constant or not increasing, then the model is overfitting and you have to retrain with a different model."],"metadata":{"id":"GrOId4J3qQCr"}},{"cell_type":"markdown","metadata":{"id":"_nMq2TJKomPO"},"source":["Q4. Is it a good idea to stop Mini-batch Gradient Descent immediately\n","when the validation error goes up?"]},{"cell_type":"markdown","source":["##No \n","##because it will be erratic in approaching the minimum (just like Stochastic Gradient Descent, but to less degree). You can always revert to the best case if the error does not improve for a while."],"metadata":{"id":"5zEsiz6XqcW6"}},{"cell_type":"markdown","metadata":{"id":"MS8QpJOeomPO"},"source":["Q5. Suppose you are using Polynomial Regression. You plot the learning\n","curves and you notice that there is a large gap between the training\n","error and the validation error. What is happening? What are three\n","ways to solve this?\n"]},{"cell_type":"markdown","source":["##This is characteristic of an overfitting model. The \"gap\" exists simply because the training error is lower than the validation error.\n","##There are three ways to solve the characteristic of an overfitting model.\n","##1)Imrpove an overfitting model is to provide more training data.\n","##2)Another tactic is to reduce the complexity of the model can be achived by reducing the number of features in the data.\n","##3)Adding regularization to the model. Either L2 (ridge regression) or L1 (lasso)."],"metadata":{"id":"2l8W_UU4qxHy"}},{"cell_type":"markdown","metadata":{"id":"eukD-6ZQomPO"},"source":["Q6. Suppose you are using Ridge Regression and you notice that the\n","training error and the validation error are almost equal and fairly\n","high. Would you say that the model suffers from high bias or high\n","variance? Should you increase the regularization hyperparameter $\\alpha$\n","or reduce it?"]},{"cell_type":"markdown","source":["##If both the training error and the validation error are almost equal and fairly high, the model is likely underfitting the training set, which means it has a \"high bias\". \n","##we have to try \"reducing\" the regularization hyperparameter or alpha."],"metadata":{"id":"9A8BUdHfrL_7"}},{"cell_type":"markdown","metadata":{"id":"fE2kBHF0omPP"},"source":["Q7. Why would you want to use:\n","   - Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?\n","   - Lasso instead of Ridge Regression?\n","   - Elastic Net instead of Lasso?"]},{"cell_type":"markdown","source":["##Ridge Regression (L2 regularization) - to avoid overfitting, to keep the weights as small as possible. Includes all the features in the model\n","##Lasso Regression instead of ridge regression - Lasso regression can zero out the coefficients, which ends up in no contribution from that particular feature, hence it does feature selection and parameter shrinkage automatically. It gives us a sparse model whereas ridge gives us a dense model.\n","##Lasso regression might not perform very well in the case of highly correlated data, hence a combination of the two, or a tradeoff, Elastic Net regression was introduced which performs fairly well with highly correlated data."],"metadata":{"id":"39hpenasrQdS"}},{"cell_type":"markdown","metadata":{"id":"QmTIz0ChomPQ"},"source":["Q8.  Can you name four of the main challenges in Machine Learning?"]},{"cell_type":"markdown","source":["##Four main challenges in Machine Learning are as follow: \n","##1)\"overfitting the data\" (using a model too complicated), \n","##2)\"underfitting the data \"(using a simple model), \n","##3)\"lacking in data\" and \n","##4)\"nonrepresentative\" data."],"metadata":{"id":"SDyNVbk2tvtM"}},{"cell_type":"markdown","metadata":{"id":"rnQa4a3momPQ"},"source":["Q9. If your model performs great on the training data but generalizes\n","poorly to new instances, what is happening? Can you name three\n","possible solutions?\n"]},{"cell_type":"markdown","source":["##When the model performs poorly on the  new data points, then it has \"overfit\" on the training data. \n","##To solve this problem , we can do any of the following three: \n","##1)Get more data, \n","##2)Implement a simpler model, \n","##3)Eliminate outliers or noise from the existing data set."],"metadata":{"id":"sHMqZgANtyNb"}},{"cell_type":"markdown","metadata":{"id":"6Gye-tNIomPQ"},"source":["Q10. What is a test set, and why would you want to use it?"]},{"cell_type":"markdown","source":["##The test set is a sub set or separate set of main data used to test the model after completing the training. \n","##It provides an unbiased final model performance metric in terms of \"accuracy\", \"precision\", etc. To put it simply, it answers the question of \"How well does the model perform?\""],"metadata":{"id":"xv1UUZXUuBhL"}},{"cell_type":"markdown","metadata":{"id":"IPjnw1m_omPR"},"source":["Q11.  What is the purpose of a validation set?"]},{"cell_type":"markdown","source":["##A validation set is a set of data used to train artificial intelligence (AI) with the goal of finding and optimizing the best model to solve a given problem. Validation sets are also known as dev sets. A supervised AI is trained on a corpus of training data."],"metadata":{"id":"GIL_mIyuuUe7"}},{"cell_type":"markdown","metadata":{"id":"1SKE2QSKomPR"},"source":["Q12. What are different loss functions? Exaplain their importance?"]},{"cell_type":"markdown","source":["Mean Square Error(MSE) = ${\\sum_{i=1}^{n}\\frac{(y_{i} - \\hat{y}_{i})^2}{n}}$"],"metadata":{"id":"8tWkM1c2A5a7"}},{"cell_type":"markdown","source":["#There areTwo types of categories of loss functions\n","#1)classification and 2)regression.\n","##Under classification\n","##1. BINARY CROSS-ENTROPY LOSS / LOG LOSS\n","This is the most common loss function used in classification problems. The   cross-entropy loss decreases as the predicted probability converges to the actual label. It measures the performance of a classification model whose predicted output is a probability value between 0 and 1.\n","\n","\n","##2.HINGE LOSS\n","The second most common loss function used for classification problems and an alternative to the cross-entropy loss function is hinge loss, primarily developed for support vector machine (SVM) model evaluation.\n","\n","\n","##under Regression Losses\n","##1.MEAN SQUARE ERROR / QUADRATIC LOSS / L2 LOSS\n","We define MSE loss function as the average of squared differences between the actual and the predicted value. It‚Äôs the most commonly used regression loss function.\n","Mean Square Error(MSE) = ${\\sum_{i=1}^{n}\\frac{(y_{i} - \\hat{y}_{i})^2}{n}}$\n","\n","##2.MEAN ABSOLUTE ERROR / L1 LOSS\n","We define MAE loss function as the average of absolute differences between the actual and the predicted value. It‚Äôs the second most commonly used regression loss function. It measures the average magnitude of errors in a set of predictions, without considering their directions.\n","(MAE) = ${\\sum_{i=1}^{n}\\frac{|y_{i} - \\hat{y}_{i}|}{n}}$\n","${\\sum_{j!=y_{i}} \\max(0,s_{j} - s{yi} + 1)}$\n","\n","\n","\n","##3.HUBER LOSS / SMOOTH MEAN ABSOLUTE ERROR\n","The Huber loss function is defined as the combination of MSE and MAE loss functions because it approaches MSE when ùõø ~ 0 and MAE when ùõø ~ ‚àû (large numbers). It is mean absolute error, which becomes quadratic when the error is small. To make the error quadratic depends on how small that error could be, which is controlled by a hyperparameter, ùõø (delta) that you can tune.\n","\n","\n","##4.LOG-COSH LOSS\n","The log-cosh loss function is defined as the logarithm of the hyperbolic cosine of the prediction error. It‚Äôs another function used in regression tasks that‚Äôs much smoother than MSE loss. It has all the advantages of Huber loss because it‚Äôs twice differentiable everywhere, unlike Huber loss, because some learning algorithms like XGBoost use Newton‚Äôs method to find the optimum, and hence the second derivative (Hessian).\n","\n","\n","##5.QUANTILE LOSS\n","A quantile is a value below which a fraction of samples in a group falls. Machine learning models work by minimizing (or maximizing) an objective function. As the name suggests, we apply the quantile regression loss function to predict quantiles. For a set of predictions, the loss will be its average."],"metadata":{"id":"blHhwhwCucib"}},{"cell_type":"markdown","metadata":{"id":"DH9vA1Z7omPR"},"source":["Q13. Explain the following:\n","    - Gradient descent\n","    - Mini-batch gradient descent\n","    - Batch gradient, and\n","    - Stochastic Gradient Descent"]},{"cell_type":"markdown","source":["#Gradient descent: is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates"],"metadata":{"id":"VoBME4Fmx7P7"}},{"cell_type":"markdown","source":["##Batch Gradient Descent: In Batch Gradient Descent, all the training data is taken into consideration to take a single step. We take the average of the gradients of all the training examples and then use that mean gradient to update our parameters. So that‚Äôs just one step of gradient descent in one epoch."],"metadata":{"id":"BN5n9jma9kld"}},{"cell_type":"markdown","source":["#Mini Batch Gradient Descent:We use a batch of a fixed number of training examples which is less than the actual dataset and call it a mini-batch.\n","#we do the following steps in one epoch:\n","\n","#1)Pick a mini-batch\n","#2)Feed it to Neural Network\n","#3)Calculate the mean gradient of the mini-batch\n","#4)Use the mean gradient we calculated in step 3 to update the weights\n","#5)Repeat steps 1‚Äì4 for the mini-batches we created\n"],"metadata":{"id":"VSK6Z46z9lFt"}},{"cell_type":"markdown","source":["# In Stochastic Gradient Descent (SGD), we consider just one example at a time to take a single step. We do the following steps in one epoch for SGD:\n","\n","#1)Take an example\n","#2)Feed it to Neural Network\n","#3)Calculate it‚Äôs gradient\n","#4)Use the gradient we calculated in step 3 to update the weights\n","#5)Repeat steps 1‚Äì4 for all the examples in training dataset"],"metadata":{"id":"g8OCl0jL9lcc"}},{"cell_type":"markdown","metadata":{"id":"Q3yYNDqmomPR"},"source":["Q14. What is learning rate?"]},{"cell_type":"markdown","source":["#The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated."],"metadata":{"id":"x_xZXCNyB8gp"}},{"cell_type":"markdown","metadata":{"id":"OTuDwkkdomPR"},"source":["Q15. Define the following terms. Explain their importance in the data analysis.\n","    - $R^2$\n","    - Adjusted $R^2$"]},{"cell_type":"markdown","source":["##$R^2$ - This tells us the degree of variance in the target variable, which is explained by our model.<br>\n","- The Total sum of squares is given by: $TSS = {\\sum({y_{i} - \\bar{y}}})^2$\n","- The Residual sum of squares is given by: $RSS = {\\sum({y_{i} - \\hat{y}}})^2$\n","- Thus, the $R^2$ is given by: $\\frac{(TSS - RSS)}{TSS}$ \n","\n","##$Adj. R^2$ - The R^2 value never decreases whenever we add a new feature. Even if the feature dosent contribute anything to the predictions, even then the R^2 value increases, hence it is flawed. So, we introduce a new stastistic, i.e, the Adjusted R-squared which takes into account the number of independent variables used for predicting the target variable."],"metadata":{"id":"sES46QQJDcUK"}},{"cell_type":"markdown","metadata":{"id":"-AU-IivOomPS"},"source":["Q16. Explain One-Hot Encoding and Label Encoding."]},{"cell_type":"markdown","source":["##1.One-hot encoding is the representation of categorical variables as binary vectors.\n","##2.Label Encoding is converting labels/words into numeric form. Using one-hot encoding increases the dimensionality of the data set. "],"metadata":{"id":"C2iumB-8Fm5B"}},{"cell_type":"markdown","metadata":{"id":"v2iai7nDomPS"},"source":["Q17. What are the assumption on Naive Bayes algorithm in classification?\n"]},{"cell_type":"markdown","source":["#Assumption1\n","##No pair of features are dependent. - The independence Assumption.\n","#Assumption2.\n","##Each feature is given the same weight(or importance). - The Equality/Identical Assumption"],"metadata":{"id":"PF_USTjPGJq4"}},{"cell_type":"markdown","metadata":{"id":"hZIq5gl5omPS"},"source":["Q18. What is the difference between classification and regression?"]},{"cell_type":"markdown","source":["##Classification is the task of predicting a discrete class label.\n","##Regression is the task of predicting a continuous quantity."],"metadata":{"id":"nnGwDqpbHEAw"}},{"cell_type":"markdown","metadata":{"id":"gxELm-wsomPS"},"source":["Q19. How to ensure that the model is not overfitting?"]},{"cell_type":"markdown","source":["##1. Split the data into test, train and valid sets.\n","##2. Cross-validation\n","##3. Larger dataset or more data, if not, then data augumentation\n","##4. Feature selection, to reduce the complexity of the model\n","##5. Regularization\n","##6. Early Stopping"],"metadata":{"id":"Sy_SkQ_sF3fJ"}},{"cell_type":"markdown","metadata":{"id":"y8yO3UmTomPS"},"source":["Q20. List the main advantage of Naive Bayes?"]},{"cell_type":"markdown","source":["#Advantages\n","##1.This algorithm works quickly and can save a lot of time. \n","\n","##2.Naive Bayes is suitable for solving multi-class prediction problems. \n","\n","##3.If its assumption of the independence of features holds true, it can perform better than other models and requires much less training data. \n","\n","##4.Naive Bayes is better suited for categorical input variables than numerical variables."],"metadata":{"id":"aiGq7TOoImcQ"}},{"cell_type":"markdown","metadata":{"id":"h8MerwJDomPS"},"source":["Q21. What you shoud do when your model is suffereing from:\n","    - Low bias and high variance?\n","    - High bias and low variance?"]},{"cell_type":"markdown","source":["# High Bias and Low Variance  \n","##Regularization\n","##Add more training data\n","# Low Bias and High Variance \n","##increasing the complexity of the model to better fit the data\n","#decreasing the regularization parameter"],"metadata":{"id":"-Hwok3LwGSoe"}},{"cell_type":"markdown","metadata":{"id":"4YfMHtWNomPT"},"source":["Q22. What is the 'Naive' in the Naive Bayes Classifier?"]},{"cell_type":"markdown","source":["#The assumption that all the features are Independent and Identically distributed and contributed equally to the predicted value is 'naive'."],"metadata":{"id":"SwTlaUalGpfQ"}},{"cell_type":"markdown","metadata":{"id":"jm6ELFlRomPT"},"source":["Q23. What is bias-variance tradeoff in Machine Learning ?"]},{"cell_type":"markdown","source":["## Ideally we want a machine learning model which takes into account all the patterns as well as the outliers in the training data and generalize them to the test data in order to achieve very small error and very high accuracy. High variance models are complex and represent all the features of the training set very well leading to minimal error on the training set but fail to generalize to the unseen data. In contrast, high bias models represent extremely simple mappings and can generalize some features to the unseen data, but the simplicity of these models leads to underfitting on the training set and generates predictions with lower variance (high bias) when applied to data outside of the training set. The ideal amount of bias and variance that a particular machine learning model should have depends on the minimization of the error (which includes bias error, variance error and noise). This is the bias-variance tradeoff."],"metadata":{"id":"DIEk8uuTKKs3"}},{"cell_type":"markdown","metadata":{"id":"MqT8k3b0omPT"},"source":["Q24. Explain different trade-offs in Machine Learning algorithms?"]},{"cell_type":"markdown","source":["##Bias-Variance tradeoff : The problem of balancing the bias and variance of the model.\n","##Precision-Recall tradeoff : Some models may have high precision but low recall and others with low precision and high recall. Good models must have a high precision and high recall"],"metadata":{"id":"V2CRwn6oHk94"}},{"cell_type":"markdown","metadata":{"id":"j2znOriyomPT"},"source":["Q25. What is cross-validation and how it is useful in traing ML models?"]},{"cell_type":"markdown","source":["#Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect overfitting, ie, failing to generalize a pattern."],"metadata":{"id":"-0IyAjNcK6WI"}},{"cell_type":"markdown","metadata":{"id":"gHVVL5hDomPT"},"source":[]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}